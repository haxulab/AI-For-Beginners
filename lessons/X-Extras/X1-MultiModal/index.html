<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Multi-Modal Networks - 人工智能初学者课程</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Multi-Modal Networks";
        var mkdocs_page_input_path = "lessons/X-Extras/X1-MultiModal/README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> 人工智能初学者课程
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../README_chs/">课程介绍</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">课程设置</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/for-teachers_chs/">给教育者</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/how-to-run_chs/">如何运行代码</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/setup_chs/">开始使用这个课程</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">人工智能简介</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/README_chs/">人工智能简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/assignment_chs/">游戏开发马拉松</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">知识表示与专家系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/README_chs/">知识表示与专家系统</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/assignment_chs/">构建一个本体</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">神经网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/README_chs/">神经网络介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/03-Perceptron/README_chs/">神经网络入门：感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/04-OwnFramework/README_chs/">神经网络介绍. 多层感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/05-Frameworks/README_chs/">神经网络框架</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">计算机视觉</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/06-IntroCV/README_chs/">计算机视觉简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >卷积神经网络</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/07-ConvNets/CNN_Architectures_chs/">著名的 CNN 架构</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/07-ConvNets/README_chs/">卷积神经网络</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >迁移学习</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/08-TransferLearning/README_chs/">预训练网络和迁移学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/08-TransferLearning/TrainingTricks_chs/">深度学习训练技巧</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/09-Autoencoders/README_chs/">自动编码器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/10-GANs/README_chs/">生成对抗网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/11-ObjectDetection/README_chs/">目标检测</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/12-Segmentation/README_chs/">分割</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">自然语言处理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/13-TextRep/README_chs/">将文本表示为张量</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/14-Embeddings/README_chs/">嵌入</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/15-LanguageModeling/README_chs/">语言建模</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/16-RNN/README_chs/">循环神经网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/17-GenerativeNetworks/README_chs/">生成网络</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/18-Transformers/READMEtransformers_chs">注意机制和Transformer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/19-NER/README_chs/">命名实体识别</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/20-LangModels/READMELargeLang_chs">预训练大语言模型</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">其他技术</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/21-GeneticAlgorithms/README_chs/">遗传算法</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/22-DeepRL/README_chs/">深度强化学习</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/23-MultiagentSystems/README_chs/">多智能体系统</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">AI伦理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../7-Ethics/README_chs/">道德与负责任的人工智能</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">多模态网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="README_chs/">多模态网络</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">人工智能初学者课程</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Multi-Modal Networks</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="multi-modal-networks">Multi-Modal Networks</h1>
<p>After the success of transformer models for solving NLP tasks, the same or similar architectures have been applied to computer vision tasks. There is a growing interest in building models that would <em>combine</em> vision and natural language capabilities. One of such attempts was done by OpenAI, and it is called CLIP and DALL.E.</p>
<h2 id="contrastive-image-pre-training-clip">Contrastive Image Pre-Training (CLIP)</h2>
<p>The main idea of CLIP is to be able to compare text prompts with an image and determine how well the image corresponds to the prompt.</p>
<p><img alt="CLIP Architecture" src="images/clip-arch.png" /></p>
<blockquote>
<p><em>Picture from <a href="https://openai.com/blog/clip/">this blog post</a></em></p>
</blockquote>
<p>The model is trained on images obtained from the Internet and their captions. For each batch, we take N pairs of (image, text), and convert them to some vector representations I<sub>1</sub>,..., I<sub>N</sub> / T<sub>1</sub>, ..., T<sub>N</sub>. Those representations are then matched together. The loss function is defined to maximize the cosine similarity between vectors corresponding to one pair (eg. I<sub>i</sub> and T<sub>i</sub>), and minimize cosine similarity between all other pairs. That is the reason this approach is called <strong>contrastive</strong>.</p>
<p>CLIP model/library is available from <a href="https://github.com/openai/CLIP">OpenAI GitHub</a>. The approach is described in <a href="https://openai.com/blog/clip/">this blog post</a>, and in more detail in <a href="https://arxiv.org/pdf/2103.00020.pdf">this paper</a>.</p>
<p>Once this model is pre-trained, we can give it a batch of images and a batch of text prompts, and it will return is the tensor with probabilities. CLIP can be used for several tasks:</p>
<p><strong>Image Classification</strong></p>
<p>Suppose we need to classify images between, say, cats, dogs and humans. In this case, we can give the model an image, and a series of text prompts: "<em>a picture of a cat</em>", "<em>a picture of a dog</em>", "<em>a picture of a human</em>". In the resulting vector of 3 probabilities we just need to select the index with a highest value.</p>
<p><img alt="CLIP for Image Classification" src="images/clip-class.png" /></p>
<blockquote>
<p><em>Picture from <a href="https://openai.com/blog/clip/">this blog post</a></em></p>
</blockquote>
<p><strong>Text-Based Image Search</strong></p>
<p>We can also do the opposite. If we have a collection of images, we can pass this collection to the model, and a text prompt - this will give us the image that is most similar to a given prompt.</p>
<h2 id="example-using-clip-for-image-classification-and-image-search">✍️ Example: <a href="Clip.ipynb">Using CLIP for Image Classification and Image Search</a></h2>
<p>Open the <a href="Clip.ipynb">Clip.ipynb</a> notebook to see CLIP in action.</p>
<h2 id="image-generation-with-vqgan-clip">Image Generation with VQGAN+ CLIP</h2>
<p>CLIP can also be used for <strong>image generation</strong> from a text prompt. In order to do this, we need a <strong>generator model</strong> that will be able to generate images based on some vector input. One of such models is called <a href="https://compvis.github.io/taming-transformers/">VQGAN</a> (Vector-Quantized GAN).</p>
<p>The main ideas of VQGAN that differentiate it from ordinary <a href="../../4-ComputerVision/10-GANs/README_chs/">GAN</a> are the following:
* Using autoregressive transformer architecture to generate a sequence of context-rich visual parts that compose the image. Those visual parts are in turn learned by <a href="../../4-ComputerVision/07-ConvNets/README_chs/">CNN</a>
* Use sub-image discriminator that detects whether parts of the image are "real" of "fake" (unlike the "all-or-nothing" approach in traditional GAN).</p>
<p>Learn more about VQGAN at the <a href="https://compvis.github.io/taming-transformers/">Taming Transformers</a> web site.</p>
<p>One of the important differences between VQGAN and traditional GAN is that the latter can produce a decent image from any input vector, while VQGAN is likely to produce an image that would not be coherent. Thus, we need to further guide the image creation process, and that can be done using CLIP. </p>
<p><img alt="VQGAN+CLIP Architecture" src="images/vqgan.png" /></p>
<p>To generate an image corresponding to a text prompt, we start with some random encoding vector that is passed through VQGAN to produce an image. Then CLIP is used to produce a loss function that shows how well the image corresponds to the text prompt. The goal then is to minimize this loss, using back propagation to adjust the input vector parameters.</p>
<p>A great library that implements VQGAN+CLIP is <a href="http://github.com/pixray/pixray">Pixray</a></p>
<table>
<thead>
<tr>
<th><img alt="Picture produced by Pixray" src="images/a_closeup_watercolor_portrait_of_young_male_teacher_of_literature_with_a_book.png" /></th>
<th><img alt="Picture produced by pixray" src="images/a_closeup_oil_portrait_of_young_female_teacher_of_computer_science_with_a_computer.png" /></th>
<th><img alt="Picture produced by Pixray" src="images/a_closeup_oil_portrait_of_old_male_teacher_of_math.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>Picture generated from prompt <em>a closeup watercolor portrait of young male teacher of literature with a book</em></td>
<td>Picture generated from prompt <em>a closeup oil portrait of young female teacher of computer science with a computer</em></td>
<td>Picture generated from prompt <em>a closeup oil portrait of old male teacher of mathematics in front of blackboard</em></td>
</tr>
</tbody>
</table>
<blockquote>
<p>Pictures from <strong>Artificial Teachers</strong> collection by <a href="http://soshnikov.com">Dmitry Soshnikov</a></p>
</blockquote>
<h2 id="dall-e">DALL-E</h2>
<h3 id="dall-e-1"><a href="https://openai.com/research/dall-e">DALL-E 1</a></h3>
<p>DALL-E is a version of GPT-3 trained to generate images from prompts. It has been trained with 12-billion parameters.</p>
<p>Unlike CLIP, DALL-E receives both text and image as a single stream of tokens for both images and text. Therefore, from multiple prompts, you can generate images based on the text.</p>
<h3 id="dall-e-2"><a href="https://openai.com/dall-e-2">DALL-E 2</a></h3>
<p>The main difference between DALL.E 1 and 2, is that it generates more realistic images and art. </p>
<p>Examples of image genrations with DALL-E:
<img alt="Picture produced by Pixray" src="images/DALL%C2%B7E%202023-06-20%2015.56.56%20-%20a%20closeup%20watercolor%20portrait%20of%20young%20male%20teacher%20of%20literature%20with%20a%20book.png" /> |  <img alt="Picture produced by pixray" src="images/DALL%C2%B7E%202023-06-20%2015.57.43%20-%20a%20closeup%20oil%20portrait%20of%20young%20female%20teacher%20of%20computer%20science%20with%20a%20computer.png" /> | <img alt="Picture produced by Pixray" src="images/DALL%C2%B7E%202023-06-20%2015.58.42%20-%20%20a%20closeup%20oil%20portrait%20of%20old%20male%20teacher%20of%20mathematics%20in%20front%20of%20blackboard.png" />
----|----|----
Picture generated from prompt <em>a closeup watercolor portrait of young male teacher of literature with a book</em> | Picture generated from prompt <em>a closeup oil portrait of young female teacher of computer science with a computer</em> | Picture generated from prompt <em>a closeup oil portrait of old male teacher of mathematics in front of blackboard</em></p>
<h2 id="references">References</h2>
<ul>
<li>VQGAN Paper: <a href="https://compvis.github.io/taming-transformers/paper/paper.pdf">Taming Transformers for High-Resolution Image Synthesis</a></li>
<li>CLIP Paper: <a href="https://arxiv.org/pdf/2103.00020.pdf">Learning Transferable Visual Models From Natural Language Supervision</a></li>
</ul>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
