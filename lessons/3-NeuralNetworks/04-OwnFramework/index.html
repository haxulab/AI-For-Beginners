<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Introduction to Neural Networks. Multi-Layered Perceptron - äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Introduction to Neural Networks. Multi-Layered Perceptron";
        var mkdocs_page_input_path = "lessons/3-NeuralNetworks/04-OwnFramework/README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../README_chs/">è¯¾ç¨‹ä»‹ç»</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">è¯¾ç¨‹è®¾ç½®</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/for-teachers_chs/">ç»™æ•™è‚²è€…</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/how-to-run_chs/">å¦‚ä½•è¿è¡Œä»£ç </a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/setup_chs/">å¼€å§‹ä½¿ç”¨è¿™ä¸ªè¯¾ç¨‹</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">äººå·¥æ™ºèƒ½ç®€ä»‹</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/README_chs/">äººå·¥æ™ºèƒ½ç®€ä»‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/assignment_chs/">æ¸¸æˆå¼€å‘é©¬æ‹‰æ¾</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">çŸ¥è¯†è¡¨ç¤ºä¸ä¸“å®¶ç³»ç»Ÿ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/README_chs/">çŸ¥è¯†è¡¨ç¤ºä¸ä¸“å®¶ç³»ç»Ÿ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/assignment_chs/">æ„å»ºä¸€ä¸ªæœ¬ä½“</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ç¥ç»ç½‘ç»œ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../README_chs/">ç¥ç»ç½‘ç»œä»‹ç»</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../03-Perceptron/README_chs/">ç¥ç»ç½‘ç»œå…¥é—¨ï¼šæ„ŸçŸ¥å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="README_chs/">ç¥ç»ç½‘ç»œä»‹ç». å¤šå±‚æ„ŸçŸ¥å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../05-Frameworks/README_chs/">ç¥ç»ç½‘ç»œæ¡†æ¶</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">è®¡ç®—æœºè§†è§‰</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/06-IntroCV/README_chs/">è®¡ç®—æœºè§†è§‰ç®€ä»‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >å·ç§¯ç¥ç»ç½‘ç»œ</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/07-ConvNets/CNN_Architectures_chs/">è‘—åçš„ CNN æ¶æ„</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/07-ConvNets/README_chs/">å·ç§¯ç¥ç»ç½‘ç»œ</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >è¿ç§»å­¦ä¹ </a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/08-TransferLearning/README_chs/">é¢„è®­ç»ƒç½‘ç»œå’Œè¿ç§»å­¦ä¹ </a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/08-TransferLearning/TrainingTricks_chs/">æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/09-Autoencoders/README_chs/">è‡ªåŠ¨ç¼–ç å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/10-GANs/README_chs/">ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/11-ObjectDetection/README_chs/">ç›®æ ‡æ£€æµ‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/12-Segmentation/README_chs/">åˆ†å‰²</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">è‡ªç„¶è¯­è¨€å¤„ç†</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/13-TextRep/README_chs/">å°†æ–‡æœ¬è¡¨ç¤ºä¸ºå¼ é‡</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/14-Embeddings/README_chs/">åµŒå…¥</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/15-LanguageModeling/README_chs/">è¯­è¨€å»ºæ¨¡</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/16-RNN/README_chs/">å¾ªç¯ç¥ç»ç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/17-GenerativeNetworks/README_chs/">ç”Ÿæˆç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/18-Transformers/READMEtransformers_chs">æ³¨æ„æœºåˆ¶å’ŒTransformer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/19-NER/README_chs/">å‘½åå®ä½“è¯†åˆ«</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/20-LangModels/READMELargeLang_chs">é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">å…¶ä»–æŠ€æœ¯</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/21-GeneticAlgorithms/README_chs/">é—ä¼ ç®—æ³•</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/22-DeepRL/README_chs/">æ·±åº¦å¼ºåŒ–å­¦ä¹ </a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/23-MultiagentSystems/README_chs/">å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">AIä¼¦ç†</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../7-Ethics/README_chs/">é“å¾·ä¸è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">å¤šæ¨¡æ€ç½‘ç»œ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../X-Extras/X1-MultiModal/README_chs/">å¤šæ¨¡æ€ç½‘ç»œ</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Introduction to Neural Networks. Multi-Layered Perceptron</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="introduction-to-neural-networks-multi-layered-perceptron">Introduction to Neural Networks. Multi-Layered Perceptron</h1>
<p>In the previous section, you learned about the simplest neural network model - one-layered perceptron, a linear two-class classification model.</p>
<p>In this section we will extend this model into a more flexible framework, allowing us to:</p>
<ul>
<li>perform <strong>multi-class classification</strong> in addition to two-class</li>
<li>solve <strong>regression problems</strong> in addition to classification</li>
<li>separate classes that are not linearly separable</li>
</ul>
<p>We will also develop our own modular framework in Python that will allow us to construct different neural network architectures.</p>
<h2 id="pre-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104">Pre-lecture quiz</a></h2>
<h2 id="formalization-of-machine-learning">Formalization of Machine Learning</h2>
<p>Let's start with formalizing the Machine Learning problem. Suppose we have a training dataset <strong>X</strong> with labels <strong>Y</strong>, and we need to build a model <em>f</em> that will make most accurate predictions. The quality of predictions is measured by <strong>Loss function</strong> &lagran;. The following loss functions are often used:</p>
<ul>
<li>For regression problem, when we need to predict a number, we can use <strong>absolute error</strong> &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, or <strong>squared error</strong> &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup></li>
<li>For classification, we use <strong>0-1 loss</strong> (which is essentially the same as <strong>accuracy</strong> of the model), or <strong>logistic loss</strong>.</li>
</ul>
<p>For one-level perceptron, function <em>f</em> was defined as a linear function <em>f(x)=wx+b</em> (here <em>w</em> is the weight matrix, <em>x</em> is the vector of input features, and <em>b</em> is bias vector). For different neural network architectures, this function can take more complex form.</p>
<blockquote>
<p>In the case of classification, it is often desirable to get probabilities of corresponding classes as network output. To convert arbitrary numbers to probabilities (eg. to normalize the output), we often use <strong>softmax</strong> function &sigma;, and the function <em>f</em> becomes <em>f(x)=&sigma;(wx+b)</em></p>
</blockquote>
<p>In the definition of <em>f</em> above, <em>w</em> and <em>b</em> are called <strong>parameters</strong> &theta;=âŸ¨<em>w,b</em>âŸ©. Given the dataset âŸ¨<strong>X</strong>,<strong>Y</strong>âŸ©, we can compute an overall error on the whole dataset as a function of parameters &theta;.</p>
<blockquote>
<p>âœ… <strong>The goal of neural network training is to minimize the error by varying parameters &theta;</strong></p>
</blockquote>
<h2 id="gradient-descent-optimization">Gradient Descent Optimization</h2>
<p>There is a well-known method of function optimization called <strong>gradient descent</strong>. The idea is that we can compute a derivative (in multi-dimensional case called <strong>gradient</strong>) of loss function with respect to parameters, and vary parameters in such a way that the error would decrease. This can be formalized as follows:</p>
<ul>
<li>Initialize parameters by some random values w<sup>(0)</sup>, b<sup>(0)</sup></li>
<li>Repeat the following step many times:<ul>
<li>w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w</li>
<li>b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b</li>
</ul>
</li>
</ul>
<p>During training, the optimization steps are supposed to be calculated considering the whole dataset (remember that loss is calculated as a sum through all training samples). However, in real life we take small portions of the dataset called <strong>minibatches</strong>, and calculate gradients based on a subset of data. Because subset is taken randomly each time, such method is called <strong>stochastic gradient descent</strong> (SGD).</p>
<h2 id="multi-layered-perceptrons-and-backpropagation">Multi-Layered Perceptrons and Backpropagation</h2>
<p>One-layer network, as we have seen above, is capable of classifying linearly separable classes. To build a richer model, we can combine several layers of the network. Mathematically it would mean that the function <em>f</em> would have a more complex form, and will be computed in several steps:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)</p>
<p>Here, &alpha; is a <strong>non-linear activation function</strong>, &sigma; is a softmax function, and parameters &theta;=&lt;<em>w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub></em>&gt;.</p>
<p>The gradient descent algorithm would remain the same, but it would be more difficult to calculate gradients. Given the chain differentiation rule, we can calculate derivatives as:</p>
<ul>
<li>&part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)</li>
<li>&part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)</li>
</ul>
<blockquote>
<p>âœ… The chain differentiation rule is used to calculate derivatives of the loss function with respect to parameters.</p>
</blockquote>
<p>Note that the left-most part of all those expressions is the same, and thus we can effectively calculate derivatives starting from the loss function and going "backwards" through the computational graph. Thus the method of training a multi-layered perceptron is called <strong>backpropagation</strong>, or 'backprop'.</p>
<p><img alt="compute graph" src="images/ComputeGraphGrad.png"/></p>
<blockquote>
<p>TODO: image citation</p>
<p>âœ… We will cover backprop in much more detail in our notebook example.  </p>
</blockquote>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson, we have built our own neural network library, and we have used it for a simple two-dimensional classification task.</p>
<h2 id="challenge">ğŸš€ Challenge</h2>
<p>In the accompanying notebook, you will implement your own framework for building and training multi-layered perceptrons. You will be able to see in detail how modern neural networks operate.</p>
<p>Proceed to the <a href="OwnFramework.ipynb">OwnFramework</a> notebook and work through it.</p>
<h2 id="post-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204">Post-lecture quiz</a></h2>
<h2 id="review-self-study">Review &amp; Self Study</h2>
<p>Backpropagation is a common algorithm used in AI and ML, worth studying <a href="https://wikipedia.org/wiki/Backpropagation">in more detail</a></p>
<h2 id="assignment"><a href="lab/README_chs/">Assignment</a></h2>
<p>In this lab, you are asked to use the framework you constructed in this lesson to solve MNIST handwritten digit classification.</p>
<ul>
<li><a href="lab/README_chs/">Instructions</a></li>
<li><a href="lab/MyFW_MNIST.ipynb">Notebook</a></li>
</ul>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
