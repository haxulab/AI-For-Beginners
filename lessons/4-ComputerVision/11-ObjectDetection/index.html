<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Object Detection - 人工智能初学者课程</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Object Detection";
        var mkdocs_page_input_path = "lessons/4-ComputerVision/11-ObjectDetection/README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> 人工智能初学者课程
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../README_chs/">课程介绍</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">课程设置</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/for-teachers_chs/">给教育者</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/how-to-run_chs/">如何运行代码</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/setup_chs/">开始使用这个课程</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">人工智能简介</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/README_chs/">人工智能简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/assignment_chs/">游戏开发马拉松</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">知识表示与专家系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/README_chs/">知识表示与专家系统</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/assignment_chs/">构建一个本体</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">神经网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/README_chs/">神经网络介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/03-Perceptron/README_chs/">神经网络入门：感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/04-OwnFramework/README_chs/">神经网络介绍. 多层感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/05-Frameworks/README_chs/">神经网络框架</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">计算机视觉</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../06-IntroCV/README_chs/">计算机视觉简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >卷积神经网络</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../07-ConvNets/CNN_Architectures_chs/">著名的 CNN 架构</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../07-ConvNets/README_chs/">卷积神经网络</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >迁移学习</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../08-TransferLearning/README_chs/">预训练网络和迁移学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../08-TransferLearning/TrainingTricks_chs/">深度学习训练技巧</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../09-Autoencoders/README_chs/">自动编码器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../10-GANs/README_chs/">生成对抗网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="README_chs/">目标检测</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../12-Segmentation/README_chs/">分割</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">自然语言处理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/13-TextRep/README_chs/">将文本表示为张量</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/14-Embeddings/README_chs/">嵌入</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/15-LanguageModeling/README_chs/">语言建模</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/16-RNN/README_chs/">循环神经网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/17-GenerativeNetworks/README_chs/">生成网络</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/18-Transformers/READMEtransformers_chs">注意机制和Transformer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/19-NER/README_chs/">命名实体识别</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/20-LangModels/READMELargeLang_chs">预训练大语言模型</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">其他技术</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/21-GeneticAlgorithms/README_chs/">遗传算法</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/22-DeepRL/README_chs/">深度强化学习</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/23-MultiagentSystems/README_chs/">多智能体系统</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">AI伦理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../7-Ethics/README_chs/">道德与负责任的人工智能</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">多模态网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../X-Extras/X1-MultiModal/README_chs/">多模态网络</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">人工智能初学者课程</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Object Detection</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="object-detection">Object Detection</h1>
<p>The image classification models we have dealt with so far took an image and produced a categorical result, such as the class 'number' in a MNIST problem. However, in many cases we do not want just to know that a picture portrays objects - we want to be able to determine their precise location. This is exactly the point of <strong>object detection</strong>.</p>
<h2 id="pre-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/111">Pre-lecture quiz</a></h2>
<p><img alt="Object Detection" src="images/Screen_Shot_2016-11-17_at_11.14.54_AM.png" /></p>
<blockquote>
<p>Image from <a href="https://pjreddie.com/darknet/yolov2/">YOLO v2 web site</a></p>
</blockquote>
<h2 id="a-naive-approach-to-object-detection">A Naive Approach to Object Detection</h2>
<p>Assuming we wanted to find a cat on a picture, a very naive approach to object detection would be the following:</p>
<ol>
<li>Break the picture down to a number of tiles</li>
<li>Run image classification on each tile.</li>
<li>Those tiles that result in sufficiently high activation can be considered to contain the object in question.</li>
</ol>
<p><img alt="Naive Object Detection" src="images/naive-detection.png" /></p>
<blockquote>
<p><em>Image from <a href="ObjectDetection-TF.ipynb">Exercise Notebook</a></em></p>
</blockquote>
<p>However, this approach is far from ideal, because it only allows the algorithm to locate the object's bounding box very imprecisely. For more precise location, we need to run some sort of <strong>regression</strong> to predict the coordinates of bounding boxes - and for that, we need specific datasets.</p>
<h2 id="regression-for-object-detection">Regression for Object Detection</h2>
<p><a href="https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491">This blog post</a> has a great gentle introduction to detecting shapes.</p>
<h2 id="datasets-for-object-detection">Datasets for Object Detection</h2>
<p>You might run across the following datasets for this task:</p>
<ul>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL VOC</a> - 20 classes</li>
<li><a href="http://cocodataset.org/#home">COCO</a> - Common Objects in Context. 80 classes, bounding boxes and segmentation masks</li>
</ul>
<p><img alt="COCO" src="images/coco-examples.jpg" /></p>
<h2 id="object-detection-metrics">Object Detection Metrics</h2>
<h3 id="intersection-over-union">Intersection over Union</h3>
<p>While for image classification it is easy to measure how well the algorithm performs, for object detection we need to measure both the correctness of the class, as well as the precision of the inferred bounding box location. For the latter, we use the so-called <strong>Intersection over Union</strong> (IoU), which measures how well two boxes (or two arbitrary areas) overlap.</p>
<p><img alt="IoU" src="images/iou_equation.png" /></p>
<blockquote>
<p><em>Figure 2 from <a href="https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">this excellent blog post on IoU</a></em></p>
</blockquote>
<p>The idea is simple - we divide the area of intersection between two figures by the area of their union. For two identical areas, IoU would be 1, while for completely disjointed areas it will be 0. Otherwise it will vary from 0 to 1. We typically only consider those bounding boxes for which IoU is over a certain value.</p>
<h3 id="average-precision">Average Precision</h3>
<p>Suppose we want to measure how well a given class of objects $C$ is recognized. To measure it, we use <strong>Average Precision</strong> metrics, which is calculated as follows:</p>
<ol>
<li>Consider Precision-Recall curve shows the accuracy depending on a detection threshold value (from 0 to 1).</li>
<li>Depending on the threshold, we will get more or less objects detected in the image, and different values of precision and recall.</li>
<li>The curve will look like this:</li>
</ol>
<p><img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecall.png"/></p>
<blockquote>
<p><em>Image from <a href="http://github.com/shwars/NeuroWorkshop">NeuroWorkshop</a></em></p>
</blockquote>
<p>The average Precision for a given class $C$ is the area under this curve. More precisely, Recall axis is typically divided into 10 parts, and Precision is averaged over all those points:</p>
<p>$$
AP = {1\over11}\sum_{i=0}^{10}\mbox{Precision}(\mbox{Recall}={i\over10})
$$</p>
<h3 id="ap-and-iou">AP and IoU</h3>
<p>We shall consider only those detections, for which IoU is above a certain value. For example, in PASCAL VOC dataset typically $\mbox{IoU Threshold} = 0.5$ is assumed, while in COCO AP is measured for different values of $\mbox{IoU Threshold}$.</p>
<p><img src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecallIoU.png"/></p>
<blockquote>
<p><em>Image from <a href="http://github.com/shwars/NeuroWorkshop">NeuroWorkshop</a></em></p>
</blockquote>
<h3 id="mean-average-precision-map">Mean Average Precision - mAP</h3>
<p>The main metric for Object Detection is called <strong>Mean Average Precision</strong>, or <strong>mAP</strong>. It is the value of Average Precision, average across all object classes, and sometimes also over $\mbox{IoU Threshold}$. In more detail, the process of calculating <strong>mAP</strong> is described
<a href="https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3">in this blog post</a>), and also <a href="https://gist.github.com/tarlen5/008809c3decf19313de216b9208f3734">here with code samples</a>.</p>
<h2 id="different-object-detection-approaches">Different Object Detection Approaches</h2>
<p>There are two broad classes of object detection algorithms:</p>
<ul>
<li><strong>Region Proposal Networks</strong> (R-CNN, Fast R-CNN, Faster R-CNN). The main idea is to generate <strong>Regions of Interests</strong> (ROI) and run CNN over them, looking for maximum activation. It is a bit similar to the naive approach, with the exception that ROIs are generated in a more clever way. One of the majors drawbacks of such methods is that they are slow, because we need many passes of the CNN classifier over the image.</li>
<li><strong>One-pass</strong> (YOLO, SSD, RetinaNet) methods. In those architectures we design the network to predict both classes and ROIs in one pass.</li>
</ul>
<h3 id="r-cnn-region-based-cnn">R-CNN: Region-Based CNN</h3>
<p><a href="http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf">R-CNN</a> uses <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf">Selective Search</a> to generate hierarchical structure of ROI regions, which are then passed through CNN feature extractors and SVM-classifiers to determine the object class, and linear regression to determine <em>bounding box</em> coordinates. <a href="https://arxiv.org/pdf/1506.01497v1.pdf">Official Paper</a></p>
<p><img alt="RCNN" src="images/rcnn1.png" /></p>
<blockquote>
<p><em>Image from van de Sande et al. ICCV’11</em></p>
</blockquote>
<p><img alt="RCNN-1" src="images/rcnn2.png" /></p>
<blockquote>
<p>*Images from <a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e">this blog</a></p>
</blockquote>
<h3 id="f-rcnn-fast-r-cnn">F-RCNN - Fast R-CNN</h3>
<p>This approach is similar to R-CNN, but regions are defined after convolution layers have been applied.</p>
<p><img alt="FRCNN" src="images/f-rcnn.png" /></p>
<blockquote>
<p>Image from <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">the Official Paper</a>, <a href="https://arxiv.org/pdf/1504.08083.pdf">arXiv</a>, 2015</p>
</blockquote>
<h3 id="faster-r-cnn">Faster R-CNN</h3>
<p>The main idea of this approach is to use neural network to predict ROIs - so-called <em>Region Proposal Network</em>. <a href="https://arxiv.org/pdf/1506.01497.pdf">Paper</a>, 2016</p>
<p><img alt="FasterRCNN" src="images/faster-rcnn.png" /></p>
<blockquote>
<p>Image from <a href="https://arxiv.org/pdf/1506.01497.pdf">the official paper</a></p>
</blockquote>
<h3 id="r-fcn-region-based-fully-convolutional-network">R-FCN: Region-Based Fully Convolutional Network</h3>
<p>This algorithm is even faster than Faster R-CNN. The main idea is the following:</p>
<ol>
<li>We extract features using ResNet-101</li>
<li>Features are processed by <strong>Position-Sensitive Score Map</strong>. Each object from $C$ classes is divided by $k\times k$ regions, and we are training to predict parts of objects.</li>
<li>For each part from $k\times k$ regions all networks vote for object classes, and the object class with maximum vote is selected.</li>
</ol>
<p><img alt="r-fcn image" src="images/r-fcn.png" /></p>
<blockquote>
<p>Image from <a href="https://arxiv.org/abs/1605.06409">official paper</a></p>
</blockquote>
<h3 id="yolo-you-only-look-once">YOLO - You Only Look Once</h3>
<p>YOLO is a realtime one-pass algorithm. The main idea is the following:</p>
<ul>
<li>Image is divided into $S\times S$ regions</li>
<li>For each region, <strong>CNN</strong> predicts $n$ possible objects, <em>bounding box</em> coordinates and <em>confidence</em>=<em>probability</em> * IoU.</li>
</ul>
<p><img alt="YOLO" src="images/yolo.png" /></p>
<blockquote>
<p>Image from <a href="https://arxiv.org/abs/1506.02640">official paper</a></p>
</blockquote>
<h3 id="other-algorithms">Other Algorithms</h3>
<ul>
<li>RetinaNet: <a href="https://arxiv.org/abs/1708.02002">official paper</a></li>
<li><a href="https://pytorch.org/vision/stable/_modules/torchvision/models/detection/retinanet.html">PyTorch Implementation in Torchvision</a></li>
<li><a href="https://github.com/fizyr/keras-retinanet">Keras Implementation</a></li>
<li><a href="https://keras.io/examples/vision/retinanet/">Object Detection with RetinaNet</a> in Keras Samples</li>
<li>SSD (Single Shot Detector): <a href="https://arxiv.org/abs/1512.02325">official paper</a></li>
</ul>
<h2 id="exercises-object-detection">✍️ Exercises: Object Detection</h2>
<p>Continue your learning in the following notebook:</p>
<p><a href="ObjectDetection.ipynb">ObjectDetection.ipynb</a></p>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson you took a whirlwind tour of all the various ways that object detection can be accomplished!</p>
<h2 id="challenge">🚀 Challenge</h2>
<p>Read through these articles and notebooks about YOLO and try them for yourself</p>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/">Good blog post</a> describing YOLO</li>
<li><a href="https://pjreddie.com/darknet/yolo/">Official site</a></li>
<li>Yolo: <a href="https://github.com/experiencor/keras-yolo2">Keras implementation</a>, <a href="https://github.com/experiencor/basic-yolo-keras/blob/master/Yolo%20Step-by-Step.ipynb">step-by-step notebook</a></li>
<li>Yolo v2: <a href="https://github.com/experiencor/keras-yolo2">Keras implementation</a>, <a href="https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb">step-by-step notebook</a></li>
</ul>
<h2 id="post-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/211">Post-lecture quiz</a></h2>
<h2 id="review-self-study">Review &amp; Self Study</h2>
<ul>
<li><a href="https://tjmachinelearning.com/lectures/1718/obj/">Object Detection</a> by Nikhil Sardana</li>
<li><a href="https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html">A good comparison of object detection algorithms</a></li>
<li><a href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852">Review of Deep Learning Algorithms for Object Detection</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/">A Step-by-Step Introduction to the Basic Object Detection Algorithms</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/">Implementation of Faster R-CNN in Python for Object Detection</a></li>
</ul>
<h2 id="assignment-object-detection"><a href="lab/README_chs/">Assignment: Object Detection</a></h2>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
