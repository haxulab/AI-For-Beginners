<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../../img/favicon.ico" />
    <title>Deep Learning Training Tricks - 人工智能初学者课程</title>
    <link rel="stylesheet" href="../../../../css/theme.css" />
    <link rel="stylesheet" href="../../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Deep Learning Training Tricks";
        var mkdocs_page_input_path = "lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../.." class="icon icon-home"> 人工智能初学者课程
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../../README_chs/">课程介绍</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">课程设置</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../0-course-setup/for-teachers_chs/">给教育者</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../0-course-setup/how-to-run_chs/">如何运行代码</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../0-course-setup/setup_chs/">开始使用这个课程</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">人工智能简介</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../1-Intro/README_chs/">人工智能简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../1-Intro/assignment_chs/">游戏开发马拉松</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">知识表示与专家系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../2-Symbolic/README_chs/">知识表示与专家系统</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../2-Symbolic/assignment_chs/">构建一个本体</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">神经网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../3-NeuralNetworks/README_chs/">神经网络介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../3-NeuralNetworks/03-Perceptron/README_chs/">神经网络入门：感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../3-NeuralNetworks/04-OwnFramework/README_chs/">神经网络介绍. 多层感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../3-NeuralNetworks/05-Frameworks/README_chs/">神经网络框架</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">计算机视觉</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../06-IntroCV/README_chs/">计算机视觉简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >卷积神经网络</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../07-ConvNets/CNN_Architectures_chs/">著名的 CNN 架构</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../07-ConvNets/README_chs/">卷积神经网络</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >迁移学习</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../README_chs/">预训练网络和迁移学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../TrainingTricks_chs/">深度学习训练技巧</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../09-Autoencoders/README_chs/">自动编码器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../10-GANs/README_chs/">生成对抗网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../11-ObjectDetection/README_chs/">目标检测</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../12-Segmentation/README_chs/">分割</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">自然语言处理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../5-NLP/13-TextRep/README_chs/">将文本表示为张量</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../5-NLP/14-Embeddings/README_chs/">嵌入</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../5-NLP/15-LanguageModeling/README_chs/">语言建模</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../5-NLP/16-RNN/README_chs/">循环神经网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../5-NLP/17-GenerativeNetworks/README_chs/">生成网络</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../5-NLP/18-Transformers/READMEtransformers_chs">注意机制和Transformer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../5-NLP/19-NER/README_chs/">命名实体识别</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../5-NLP/20-LangModels/READMELargeLang_chs">预训练大语言模型</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">其他技术</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../6-Other/21-GeneticAlgorithms/README_chs/">遗传算法</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../6-Other/22-DeepRL/README_chs/">深度强化学习</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../6-Other/23-MultiagentSystems/README_chs/">多智能体系统</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">AI伦理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../7-Ethics/README_chs/">道德与负责任的人工智能</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">多模态网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../X-Extras/X1-MultiModal/README_chs/">多模态网络</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../..">人工智能初学者课程</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Deep Learning Training Tricks</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="deep-learning-training-tricks">Deep Learning Training Tricks</h1>
<p>As neural networks become deeper, the process of their training becomes more and more challenging. One major problem is so-called <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradients</a> or <a href="https://deepai.org/machine-learning-glossary-and-terms/exploding-gradient-problem#:~:text=Exploding%20gradients%20are%20a%20problem,updates%20are%20small%20and%20controlled.">exploding gradients</a>. <a href="https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11">This post</a> gives a good introduction into those problems.</p>
<p>To make training deep networks more efficient, there are a few techniques that can be used.</p>
<h2 id="keeping-values-in-reasonable-interval">Keeping values in reasonable interval</h2>
<p>To make numerical computations more stable, we want to make sure that all values within our neural network are within reasonable scale, typically [-1..1] or [0..1]. It is not a very strict requirement, but the nature of floating point computations is such that values of different magnitudes cannot be accurately manipulated together. For example, if we add 10<sup>-10</sup> and 10<sup>10</sup>, we are likely to get 10<sup>10</sup>, because smaller value would be "converted" to the same order as the larger one, and thus mantissa would be lost.</p>
<p>Most activation functions have non-linearities around [-1..1], and thus it makes sense to scale all input data to [-1..1] or [0..1] interval.</p>
<h2 id="initial-weight-initialization">Initial Weight Initialization</h2>
<p>Ideally, we want the values to be in the same range after passing through network layers. Thus it is important to initialize weights in such a way as to preserve the distribution of values.</p>
<p>Normal distribution <strong>N(0,1)</strong> is not a good idea, because if we have <em>n</em> inputs, the standard deviation of output would be <em>n</em>, and values are likely to jump out of [0..1] interval.</p>
<p>The following initializations are often used:</p>
<ul>
<li>Uniform distribution -- <code>uniform</code></li>
<li><strong>N(0,1/n)</strong> -- <code>gaussian</code></li>
<li><strong>N(0,1/&radic;n_in)</strong> guarantees that for inputs with zero mean and standard deviation of 1 the same mean/standard deviation would remain</li>
<li><strong>N(0,&radic;2/(n_in+n_out))</strong> -- so-called <strong>Xavier initialization</strong> (<code>glorot</code>), it helps to keep the signals in range during both forward and backward propagation</li>
</ul>
<h2 id="batch-normalization">Batch Normalization</h2>
<p>Even with proper weight initialization, weights can get arbitrary big or small during the training, and they will bring signals out of proper range. We can bring signals back by using one of <strong>normalization</strong> techniques. While there are several of them (Weight normalization, Layer Normalization), the most often used is Batch Normalization.</p>
<p>The idea of <strong>batch normalization</strong> is to take into account all values across the minibatch, and perform normalization (i.e. subtract mean and divide by standard deviation) based on those values. It is implemented as a network layer that does this normalization after applying the weights, but before activation function. As a result, we are likely to see higher final accuracy and faster training.</p>
<p>Here is the <a href="https://arxiv.org/pdf/1502.03167.pdf">original paper</a> on batch normalization, the <a href="https://en.wikipedia.org/wiki/Batch_normalization">explanation on Wikipedia</a>, and <a href="https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338">a good introductory blog post</a> (and the one <a href="https://habrahabr.ru/post/309302/">in Russian</a>).</p>
<h2 id="dropout">Dropout</h2>
<p><strong>Dropout</strong> is an interesting technique that removes a certain percentage of random neurons during training. It is also implemented as a layer with one parameter (percentage of neurons to remove, typically 10%-50%), and during training it zeroes random elements of the input vector, before passing it to the next layer.</p>
<p>While this may sound like a strange idea, you can see the effect of dropout on training MNIST digit classifier in <a href="../Dropout.ipynb"><code>Dropout.ipynb</code></a> notebook. It speeds up training and allows us to achieve higher accuracy in less training epochs.</p>
<p>This effect can be explained in several ways:</p>
<ul>
<li>It can be considered to be a random shocking factor to the model, which takes optimiation out of local minimum</li>
<li>It can be considered as <em>implicit model averaging</em>, because we can say that during dropout we are training slightly different model</li>
</ul>
<blockquote>
<p><em>Some people say that when a drunk person tries to learn something, he will remember this better next morning, comparing to a sober person, because a brain with some malfunctioning neurons tries to adapt better to gasp the meaning. We never tested ourselves if this is true of not</em></p>
</blockquote>
<h2 id="preventing-overfitting">Preventing overfitting</h2>
<p>One of the very important aspect of deep learning is too be able to prevent <a href="../../3-NeuralNetworks/05-Frameworks/Overfitting_chs.md">overfitting</a>. While it might be tempting to use very powerful neural network model, we should always balance the number of model parameters with the number of training samples.</p>
<blockquote>
<p>Make sure you understand the concept of <a href="../../3-NeuralNetworks/05-Frameworks/Overfitting_chs.md">overfitting</a> we have introduced earlier!</p>
</blockquote>
<p>There are several ways to prevent overfitting:</p>
<ul>
<li>Early stopping -- continuously monitor error on validation set and stopping training when validation error starts to increase.</li>
<li>Explicit Weight Decay / Regularization -- adding an extra penalty to the loss function for high absolute values of weights, which prevents the model of getting very unstable results</li>
<li>Model Averaging -- training several models and then averaging the result. This helps to minimize the variance.</li>
<li>Dropout (Implicit Model Averaging)</li>
</ul>
<h2 id="optimizers-training-algorithms">Optimizers / Training Algorithms</h2>
<p>Another important aspect of training is to chose good training algorithm. While classical <strong>gradient descent</strong> is a reasonable choice, it can sometimes be too slow, or result in other problems.</p>
<p>In deep learning, we use <strong>Stochastic Gradient Descent</strong> (SGD), which is a gradient descent applied to minibatches, randomly selected from the training set. Weights are adjusted using this formula:</p>
<p>w<sup>t+1</sup> = w<sup>t</sup> - &eta;&nabla;&lagran;</p>
<h3 id="momentum">Momentum</h3>
<p>In <strong>momentum SGD</strong>, we are keeping a portion of a gradient from previous steps. It is similar to when we are moving somewhere with inertia, and we receive a punch in a different direction, our trajectory does not change immediately, but keeps some part of the original movement. Here we introduce another vector v to represent the <em>speed</em>:</p>
<ul>
<li>v<sup>t+1</sup> = &gamma; v<sup>t</sup> - &eta;&nabla;&lagran;</li>
<li>w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup></li>
</ul>
<p>Here parameter &gamma; indicates the extent to which we take inertia into account: &gamma;=0 corresponds to classical SGD; &gamma;=1 is a pure motion equation.</p>
<h3 id="adam-adagrad-etc">Adam, Adagrad, etc.</h3>
<p>Since in each layer we multiply signals by some matrix W<sub>i</sub>, depending on ||W<sub>i</sub>||, the gradient can either diminish and be close to 0, or rise indefinitely. It is the essence of Exploding/Vanishing Gradients problem.</p>
<p>One of the solutions to this problem is to use only direction of the gradient in the equation, and ignore the absolute value, i.e.</p>
<p>w<sup>t+1</sup> = w<sup>t</sup> - &eta;(&nabla;&lagran;/||&nabla;&lagran;||), where ||&nabla;&lagran;|| = &radic;&sum;(&nabla;&lagran;)<sup>2</sup></p>
<p>This algorithm is called <strong>Adagrad</strong>. Another algorithms that use the same idea: <strong>RMSProp</strong>, <strong>Adam</strong></p>
<blockquote>
<p><strong>Adam</strong> is considered to be a very efficient algorithm for many applications, so if you are not sure which one to use - use Adam.</p>
</blockquote>
<h3 id="gradient-clipping">Gradient clipping</h3>
<p>Gradient clipping is an extension the idea above. When the ||&nabla;&lagran;|| &le; &theta;, we consider the original gradient in the weight optimization, and when ||&nabla;&lagran;|| &gt; &theta; - we divide the gradient by it's norm. Here &theta; is a parameter, in most cases we can take &theta;=1 or &theta;=10.</p>
<h3 id="learning-rate-decay">Learning rate decay</h3>
<p>Training success often depends on the learning rate parameter &eta;. It is logical to assume that larger values of &eta; result in faster training, which is something we typically want in the beginning of the training, and then smaller value of &eta; allow us to fine-tune the network. Thus, in most of the cases we want to decrease &eta; in the process of the training.</p>
<p>This can be done by multiplying &eta; by some number (eg. 0.98) after each epoch of the training, or by using more complicated <strong>learning rate schedule</strong>.</p>
<h2 id="different-network-architectures">Different Network Architectures</h2>
<p>Selecting right network architecture for your problem can be tricky. Normally, we would take an architecture that has proven to work for our specific task (or similar one). Here is a <a href="https://www.topbots.com/a-brief-history-of-neural-network-architectures/">good overview</a> or neural network architectures for computer vision.</p>
<blockquote>
<p>It is important to select an architecture that will be powerful enough for the number of training samples that we have. Selecting too powerful model can result in <a href="../../3-NeuralNetworks/05-Frameworks/Overfitting_chs.md">overfitting</a></p>
</blockquote>
<p>Another good way would be to use and architecture that will automatically adjust to the required complexity. To some extent, <strong>ResNet</strong> architecture and <strong>Inception</strong> are self-adjusting. <a href="../../07-ConvNets/CNN_Architectures_chs/">More on computer vision architectures</a></p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../../..";</script>
    <script src="../../../../js/theme_extra.js"></script>
    <script src="../../../../js/theme.js"></script>
      <script src="../../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
