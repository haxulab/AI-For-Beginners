<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Generative Adversarial Networks - äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Generative Adversarial Networks";
        var mkdocs_page_input_path = "lessons/4-ComputerVision/10-GANs/README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../README_chs/">è¯¾ç¨‹ä»‹ç»</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">è¯¾ç¨‹è®¾ç½®</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/for-teachers_chs/">ç»™æ•™è‚²è€…</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/how-to-run_chs/">å¦‚ä½•è¿è¡Œä»£ç </a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/setup_chs/">å¼€å§‹ä½¿ç”¨è¿™ä¸ªè¯¾ç¨‹</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">äººå·¥æ™ºèƒ½ç®€ä»‹</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/README_chs/">äººå·¥æ™ºèƒ½ç®€ä»‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/assignment_chs/">æ¸¸æˆå¼€å‘é©¬æ‹‰æ¾</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">çŸ¥è¯†è¡¨ç¤ºä¸ä¸“å®¶ç³»ç»Ÿ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/README_chs/">çŸ¥è¯†è¡¨ç¤ºä¸ä¸“å®¶ç³»ç»Ÿ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/assignment_chs/">æ„å»ºä¸€ä¸ªæœ¬ä½“</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ç¥ç»ç½‘ç»œ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/README_chs/">ç¥ç»ç½‘ç»œä»‹ç»</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/03-Perceptron/README_chs/">ç¥ç»ç½‘ç»œå…¥é—¨ï¼šæ„ŸçŸ¥å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/04-OwnFramework/README_chs/">ç¥ç»ç½‘ç»œä»‹ç». å¤šå±‚æ„ŸçŸ¥å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/05-Frameworks/README_chs/">ç¥ç»ç½‘ç»œæ¡†æ¶</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">è®¡ç®—æœºè§†è§‰</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../06-IntroCV/README_chs/">è®¡ç®—æœºè§†è§‰ç®€ä»‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >å·ç§¯ç¥ç»ç½‘ç»œ</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../07-ConvNets/CNN_Architectures_chs/">è‘—åçš„ CNN æ¶æ„</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../07-ConvNets/README_chs/">å·ç§¯ç¥ç»ç½‘ç»œ</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >è¿ç§»å­¦ä¹ </a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../08-TransferLearning/README_chs/">é¢„è®­ç»ƒç½‘ç»œå’Œè¿ç§»å­¦ä¹ </a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../08-TransferLearning/TrainingTricks_chs/">æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../09-Autoencoders/README_chs/">è‡ªåŠ¨ç¼–ç å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="README_chs/">ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../11-ObjectDetection/README_chs/">ç›®æ ‡æ£€æµ‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../12-Segmentation/README_chs/">åˆ†å‰²</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">è‡ªç„¶è¯­è¨€å¤„ç†</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/13-TextRep/README_chs/">å°†æ–‡æœ¬è¡¨ç¤ºä¸ºå¼ é‡</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/14-Embeddings/README_chs/">åµŒå…¥</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/15-LanguageModeling/README_chs/">è¯­è¨€å»ºæ¨¡</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/16-RNN/README_chs/">å¾ªç¯ç¥ç»ç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/17-GenerativeNetworks/README_chs/">ç”Ÿæˆç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/18-Transformers/READMEtransformers_chs">æ³¨æ„æœºåˆ¶å’ŒTransformer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/19-NER/README_chs/">å‘½åå®ä½“è¯†åˆ«</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/20-LangModels/READMELargeLang_chs">é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">å…¶ä»–æŠ€æœ¯</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/21-GeneticAlgorithms/README_chs/">é—ä¼ ç®—æ³•</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/22-DeepRL/README_chs/">æ·±åº¦å¼ºåŒ–å­¦ä¹ </a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/23-MultiagentSystems/README_chs/">å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">AIä¼¦ç†</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../7-Ethics/README_chs/">é“å¾·ä¸è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">å¤šæ¨¡æ€ç½‘ç»œ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../X-Extras/X1-MultiModal/README_chs/">å¤šæ¨¡æ€ç½‘ç»œ</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Generative Adversarial Networks</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="generative-adversarial-networks">Generative Adversarial Networks</h1>
<p>In the previous section, we learned about <strong>generative models</strong>: models that can generate new images similar to the ones in the training dataset. VAE was a good example of a generative model.</p>
<h2 id="pre-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/110">Pre-lecture quiz</a></h2>
<p>However, if we try to generate something really meaningful, like a painting at reasonable resolution, with VAE, we will see that training does not converge well. For this use case, we should learn about another architecture specifically targeted at generative models - <strong>Generative Adversarial Networks</strong>, or GANs.</p>
<p>The main idea of a GAN is to have two neural networks that will be trained against each other:</p>
<p><img src="images/gan_architecture.png" width="70%"/></p>
<blockquote>
<p>Image by <a href="http://soshnikov.com">Dmitry Soshnikov</a></p>
<p>âœ… A little vocabulary:
* <strong>Generator</strong> is a network that takes some random vector, and produces the image as a result
* <strong>Discriminator</strong> is a network that takes an image, and it should tell whether it is a real image (from training dataset), or it was generated by a generator. It is essentially an image classifier.</p>
</blockquote>
<h3 id="discriminator">Discriminator</h3>
<p>The architecture of discriminator does not differ from an ordinary image classification network. In the simplest case it can be fully-connected classifier, but most probably it will be a <a href="../07-ConvNets/README_chs/">convolutional network</a>.</p>
<blockquote>
<p>âœ… A GAN based on convolutional networks is called a <a href="https://arxiv.org/pdf/1511.06434.pdf">DCGAN</a></p>
</blockquote>
<p>A CNN discriminator consists of the following layers: several convolutions+poolings (with decreasing spatial size) and, one-or-more fully-connected layers to get "feature vector", final binary classifier.</p>
<blockquote>
<p>âœ… A 'pooling' in this context is a technique that reduces the size of the image. "Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer." - <a href="https://wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers">source</a></p>
</blockquote>
<h3 id="generator">Generator</h3>
<p>A Generator is slightly more tricky. You can consider it to be a reversed discriminator. Starting from a latent vector (in place of a feature vector), it has a fully-connected layer to convert it into the required size/shape, followed by deconvolutions+upscaling. This is similar to <em>decoder</em> part of <a href="../09-Autoencoders/README_chs/">autoencoder</a>.</p>
<blockquote>
<p>âœ… Because the convolution layer is implemented as a linear filter traversing the image, deconvolution is essentially similar to convolution, and can be implemented using the same layer logic.</p>
</blockquote>
<p><img src="images/gan_arch_detail.png" width="70%"/></p>
<blockquote>
<p>Image by <a href="http://soshnikov.com">Dmitry Soshnikov</a></p>
</blockquote>
<h3 id="training-the-gan">Training the GAN</h3>
<p>GANs are called <strong>adversarial</strong> because there is a constant competition between the generator and the discriminator. During this competition, both generator and discriminator improve, thus the network learns to produce better and better pictures.</p>
<p>The training happens in two stages:</p>
<ul>
<li><strong>Training the discriminator</strong>. This task is pretty straightforward: we generate a batch of images by the generator, labeling them 0, which stands for fake image, and taking a batch of images from the input dataset (with label 1, real image). We obtain some <em>discriminator loss</em>, and perform backprop.</li>
<li><strong>Training the generator</strong>. This is slightly more tricky, because we do not know the expected output for the generator directly. We take the whole GAN network consisting of a generator followed by discriminator, feed it with some random vectors, and expect the result to be 1 (corresponding to real images). We then freeze the parameters of the discriminator (we do not want it to be trained at this step), and perform the backprop.</li>
</ul>
<p>During this process, both the generator and the discriminator losses are not going down significantly. In the ideal situation, they should oscillate, corresponding to both networks improving their performance.</p>
<h2 id="exercises-gans">âœï¸ Exercises: GANs</h2>
<ul>
<li><a href="GANTF.ipynb">GAN Notebook in TensorFlow/Keras</a></li>
<li><a href="GANPyTorch.ipynb">GAN Notebook in PyTorch</a></li>
</ul>
<h3 id="problems-with-gan-training">Problems with GAN training</h3>
<p>GANs are known to be especially difficult to train. Here are a few problems:</p>
<ul>
<li><strong>Mode Collapse</strong>. By this term we mean that the generator learns to produce one successful image that tricks the generator, and not a variety of different images.</li>
<li><strong>Sensitivity to hyperparameters</strong>. Often you can see that a GAN does not converge at all, and then suddenly decreases in the learning rate leading to convergence.</li>
<li>Keeping a <strong>balance</strong> between the generator and the discriminator. In many cases discriminator loss can drop to zero relatively quickly, which results in the generator being unable to train further. To overcome this, we can try setting different learning rates for the generator and discriminator, or skip discriminator training if the loss is already too low.</li>
<li>Training for <strong>high resolution</strong>. Reflecting the same problem as with autoencoders, this problem is triggered because reconstructing too many layers of convolutional network leads to artifacts. This problem is typically solved with so-called <strong>progressive growing</strong>, when first a few layers are trained on low-res images, and then layers are "unblocked" or added. Another solution would be adding extra connections between layers and training several resolutions at once - see this <a href="https://arxiv.org/abs/1903.06048">Multi-Scale Gradient GANs paper</a> for details.</li>
</ul>
<h2 id="style-transfer">Style Transfer</h2>
<p>GANs is a great way to generate artistic images. Another interesting technique is so-called <strong>style transfer</strong>, which takes one <strong>content image</strong>, and re-draws it in a different style, applying filters from <strong>style image</strong>. </p>
<p>The way it works is the following:
* We start with a random noise image (or with a content image, but for the sake of understanding it is easier to start from random noise)
* Our goal would be to create such an image, that would be close to both content image and style image. This would be determined by two loss functions:
   - <strong>Content loss</strong> is computed based on the features extracted by the CNN at some layers from current image and content image
   - <strong>Style loss</strong> is computed between current image and style image in a clever way using Gram matrices (more details in the <a href="StyleTransfer.ipynb">example notebook</a>)
* To make the image smoother and remove noise, we also introduce <strong>Variation loss</strong>, which computes average distance between neighboring pixels
* The main optimization loop adjusts current image using gradient descent (or some other optimization algorithm) to minimize the total loss, which is a weighted sum of all three losses. </p>
<h2 id="example-style-transfer">âœï¸ Example: <a href="StyleTransfer.ipynb">Style Transfer</a></h2>
<h2 id="post-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/210">Post-lecture quiz</a></h2>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson, you learned about GANS and how to train them. You also learned about the special challenges that this type of Neural Network can face, and some strategies on how to move past them.</p>
<h2 id="challenge">ğŸš€ Challenge</h2>
<p>Run through the <a href="StyleTransfer.ipynb">Style Transfer notebook</a> using your own images.</p>
<h2 id="review-self-study">Review &amp; Self Study</h2>
<p>For reference, read more about GANs in these resources:</p>
<ul>
<li>Marco Pasini, <a href="https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628">10 Lessons I Learned Training GANs for one Year</a></li>
<li><a href="https://en.wikipedia.org/wiki/StyleGAN">StyleGAN</a>, a <em>de facto</em> GAN architecture to consider</li>
<li><a href="https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/">Creating Generative Art using GANs on Azure ML</a></li>
</ul>
<h2 id="assignment">Assignment</h2>
<p>Revisit one of the two notebooks associated to this lesson and retrain the GAN on your own images. What can you create?</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
