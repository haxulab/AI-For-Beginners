# 深度强化学习

强化学习（RL）被认为是与监督学习和无监督学习并列的基本机器学习范式之一。在监督学习中，我们依赖于具有已知结果的数据集，而RL则基于**通过做来学习**。例如，当我们第一次看到计算机游戏时，即使不知道规则，我们也会开始玩，并且很快就通过游戏过程和调整自己的行为提升游戏技能。

## [课前测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122)

要执行强化学习，我们需要：

* 一个**环境**或**模拟器**，它制定游戏的规则。我们应该能够在模拟器中运行实验并观察结果。
* 某种**奖励函数**，它指出我们的实验有多成功。在学习玩计算机游戏的情况下，奖励将是我们的最终得分。

基于奖励函数，我们应该能够调整自己的行为并提高技能，以便下次玩得更好。其他类型的机器学习与RL的主要区别在于，在RL中我们通常不知道我们是否赢了直到游戏结束。因此，我们无法判断单独某一步是否是好的一步——我们只能在游戏结束后获得奖励。

在RL过程中，我们通常会进行许多实验。在每次实验中，我们需要在遵循我们迄今为止学到的最佳策略（**利用**）和探索新的可能状态（**探索**）之间取得平衡。

## OpenAI Gym

一个很好的RL工具是 [OpenAI Gym](https://gym.openai.com/) ——一个**模拟环境**，它可以模拟从Atari游戏到杠杆平衡背后的物理在内的许多不同环境。这是训练强化学习算法最流行的模拟环境之一，由[OpenAI](https://openai.com/)维护。

> **注意**: 你可以在[这里](https://gym.openai.com/envs/#classic_control)查看OpenAI Gym提供的所有环境。

## 小车倒立摆平衡

你可能都见过现代平衡装置，例如*赛格威*或*陀螺车*。它们通过根据加速度计或陀螺仪的信号调整车轮自动平衡。在本节中，我们将学习如何解决类似的问题——平衡一个杆。这类似于杂技演员需要在手上平衡一个杆的情况——但这种杆的平衡仅在1D中进行。

一种简化的平衡版本被称为**小车倒立摆**问题。在小车倒立摆世界中，我们有一个可以左右移动的水平滑块，目标是在滑块移动时在其顶部保持竖直杆的平衡。

<img alt="小车倒立摆" src="images/cartpole.png" width="200"/>

创建并使用这个环境，我们需要几行Python代码：

```python
import gym
env = gym.make("CartPole-v1")

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observation, reward, done, info = env.step(action)
   total_reward += reward

print(f"Total reward: {total_reward}")
```

每个环境可以完全相同地访问：
* `env.reset` 启动一个新实验
* `env.step` 执行一次模拟步骤。它从**动作空间**接收一个**动作**，并返回一个**观察值**（来自观察空间），以及一个奖励和终止标志。

在上述示例中，我们在每一步执行一个随机动作，这就是实验寿命非常短的原因：

![平衡失败的小车倒立摆](images/cartpole-nobalance.gif)

RL算法的目标是训练一个模型——所谓的**策略** &pi;——它将在给定状态下返回动作。我们也可以认为策略是概率的，例如，对于任何状态 *s* 和动作 *a*，它将返回我们在状态 *s* 下应采取 *a* 的概率 &pi;(*a*|*s*)。

## 策略梯度算法

最明显的建模策略方法是创建一个神经网络，它将状态作为输入，并返回相应的动作（或者更确切地说，所有动作的概率）。在某种意义上，它类似于一个普通的分类任务，主要区别在于我们不知道在每一步应该采取哪些动作。

这里的想法是估计这些概率。我们构建一个**累积奖励**向量，显示实验中每一步的总奖励。我们还通过乘以某个系数 &gamma;=0.99 来应用**奖励折现**，以减少早期奖励的作用。然后，我们加强沿实验路径产生较大奖励的步骤。

> 了解关于策略梯度算法的更多信息，并在[示例笔记本](CartPole-RL-TF.ipynb)中看到它的实际应用。

## Actor-Critic算法

策略梯度方法的改进版本称为**Actor-Critic**。其主要思想是神经网络将被训练同时返回两件事：

* 策略，决定采取哪个动作。这部分称为**actor**
* 我们可以在该状态下预计获得的总奖励估计——这一部分称为**critic**。

在某种意义上，这种架构类似于[生成对抗网络](../../4-ComputerVision/10-GANs/README.md)，其中有两个相互对抗训练的网络。在actor-critic模型中，actor提出我们需要采取的动作，critic尝试批评并估计结果。然而，我们的目标是联合训练这些网络。

因为我们在实验中知道真实的累积奖励和critic返回的结果，构建一个最小化它们之间差异的损失函数相对容易。这将给我们**critic损失**。我们可以使用策略梯度算法中的相同方法计算**actor损失**。

运行这些算法之一后，我们可以期望我们的小车倒立摆表现如下：

![平衡成功的小车倒立摆](images/cartpole-balance.gif)

## ✍️ 练习：策略梯度和Actor-Critic强化学习

在以下笔记本中继续你的学习：

* [TensorFlow中的强化学习](CartPole-RL-TF.ipynb)
* [PyTorch中的强化学习](CartPole-RL-PyTorch.ipynb)

## 其他强化学习任务

如今，强化学习是一个快速发展的研究领域。一些有趣的强化学习示例包括：

* 教计算机玩**Atari游戏**。这个问题的挑战在于，我们没有简单的状态表示为向量，而是一个截图——我们需要使用CNN将屏幕图像转化为特征向量，或提取奖励信息。Atari游戏在Gym中可用。
* 教计算机下棋或围棋等桌面游戏。最近，像**Alpha Zero**这样的最新程序是通过两个代理相互对战并在每一步提升自己从零开始训练的。
* 在工业中，RL用于从模拟中创建控制系统。一个名为[Bonsai](https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste)的服务专门为此设计。

## 结论

现在我们已学习如何通过为代理提供奖励函数定义游戏所需状态，并给他们智能探索搜索空间的机会来训练代理以获得良好的结果。我们已经成功尝试了两种算法，并在相对较短的时间内获得了不错的结果。然而，这只是你RL之旅的开始，如果你想深入研究，绝对应该考虑攻读一个单独的课程。

## 🚀 挑战

探索“其他强化学习任务”部分列出的应用，并尝试实现其中一个！

## [课后测验](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222)

## 回顾与自学

在我们的[初学者机器学习课程](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md)中了解更多关于经典强化学习的内容。

观看[这个精彩的视频](https://www.youtube.com/watch?v=qv6UVOQ0F44)，了解一台计算机如何学习玩Super Mario。

## 作业: [训练山地车](lab/README.md)

在这个作业中，你的目标是训练一个不同的Gym环境——[Mountain Car](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)。
