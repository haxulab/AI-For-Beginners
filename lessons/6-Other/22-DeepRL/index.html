<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Deep Reinforcement Learning - äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Deep Reinforcement Learning";
        var mkdocs_page_input_path = "lessons/6-Other/22-DeepRL/README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../README_chs/">è¯¾ç¨‹ä»‹ç»</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">è¯¾ç¨‹è®¾ç½®</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/for-teachers_chs/">ç»™æ•™è‚²è€…</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/how-to-run_chs/">å¦‚ä½•è¿è¡Œä»£ç </a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/setup_chs/">å¼€å§‹ä½¿ç”¨è¿™ä¸ªè¯¾ç¨‹</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">äººå·¥æ™ºèƒ½ç®€ä»‹</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/README_chs/">äººå·¥æ™ºèƒ½ç®€ä»‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/assignment_chs/">æ¸¸æˆå¼€å‘é©¬æ‹‰æ¾</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">çŸ¥è¯†è¡¨ç¤ºä¸ä¸“å®¶ç³»ç»Ÿ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/README_chs/">çŸ¥è¯†è¡¨ç¤ºä¸ä¸“å®¶ç³»ç»Ÿ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/assignment_chs/">æ„å»ºä¸€ä¸ªæœ¬ä½“</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ç¥ç»ç½‘ç»œ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/README_chs/">ç¥ç»ç½‘ç»œä»‹ç»</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/03-Perceptron/README_chs/">ç¥ç»ç½‘ç»œå…¥é—¨ï¼šæ„ŸçŸ¥å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/04-OwnFramework/README_chs/">ç¥ç»ç½‘ç»œä»‹ç». å¤šå±‚æ„ŸçŸ¥å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/05-Frameworks/README_chs/">ç¥ç»ç½‘ç»œæ¡†æ¶</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">è®¡ç®—æœºè§†è§‰</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/06-IntroCV/README_chs/">è®¡ç®—æœºè§†è§‰ç®€ä»‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >å·ç§¯ç¥ç»ç½‘ç»œ</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/07-ConvNets/CNN_Architectures_chs/">è‘—åçš„ CNN æ¶æ„</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/07-ConvNets/README_chs/">å·ç§¯ç¥ç»ç½‘ç»œ</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >è¿ç§»å­¦ä¹ </a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/08-TransferLearning/README_chs/">é¢„è®­ç»ƒç½‘ç»œå’Œè¿ç§»å­¦ä¹ </a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/08-TransferLearning/TrainingTricks_chs/">æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/09-Autoencoders/README_chs/">è‡ªåŠ¨ç¼–ç å™¨</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/10-GANs/README_chs/">ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/11-ObjectDetection/README_chs/">ç›®æ ‡æ£€æµ‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/12-Segmentation/README_chs/">åˆ†å‰²</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">è‡ªç„¶è¯­è¨€å¤„ç†</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/13-TextRep/README_chs/">å°†æ–‡æœ¬è¡¨ç¤ºä¸ºå¼ é‡</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/14-Embeddings/README_chs/">åµŒå…¥</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/15-LanguageModeling/README_chs/">è¯­è¨€å»ºæ¨¡</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/16-RNN/README_chs/">å¾ªç¯ç¥ç»ç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/17-GenerativeNetworks/README_chs/">ç”Ÿæˆç½‘ç»œ</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/18-Transformers/READMEtransformers_chs">æ³¨æ„æœºåˆ¶å’ŒTransformer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5-NLP/19-NER/README_chs/">å‘½åå®ä½“è¯†åˆ«</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../5-NLP/20-LangModels/READMELargeLang_chs">é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">å…¶ä»–æŠ€æœ¯</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../21-GeneticAlgorithms/README_chs/">é—ä¼ ç®—æ³•</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="README_chs/">æ·±åº¦å¼ºåŒ–å­¦ä¹ </a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../23-MultiagentSystems/README_chs/">å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">AIä¼¦ç†</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../7-Ethics/README_chs/">é“å¾·ä¸è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">å¤šæ¨¡æ€ç½‘ç»œ</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../X-Extras/X1-MultiModal/README_chs/">å¤šæ¨¡æ€ç½‘ç»œ</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">äººå·¥æ™ºèƒ½åˆå­¦è€…è¯¾ç¨‹</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Deep Reinforcement Learning</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="deep-reinforcement-learning">Deep Reinforcement Learning</h1>
<p>Reinforcement learning (RL) is seen as one of the basic machine learning paradigms, next to supervised learning and unsupervised learning. While in supervised learning we rely on the dataset with known outcomes, RL is based on <strong>learning by doing</strong>. For example, when we first see a computer game, we start playing, even without knowing the rules, and soon we are able to improve our skills just by the process of playing and adjusting our behavior.</p>
<h2 id="pre-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/122">Pre-lecture quiz</a></h2>
<p>To perform RL, we need:</p>
<ul>
<li>An <strong>environment</strong> or <strong>simulator</strong> that sets the rules of the game. We should be able to run the experiments in the simulator and observe the results.</li>
<li>Some <strong>Reward function</strong>, which indicate how successful our experiment was. In case of learning to play a computer game, the reward would be our final score.</li>
</ul>
<p>Based on the reward function, we should be able to adjust our behavior and improve our skills, so that the next time we play better. The main difference between other types of machine learning and RL is that in RL we typically do not know whether we win or lose until we finish the game. Thus, we cannot say whether a certain move alone is good or not - we only receive a reward at the end of the game.</p>
<p>During RL, we typically perform many experiments. During each experiment, we need to balance between following the optimal strategy that we have learned so far (<strong>exploitation</strong>) and exploring new possible states (<strong>exploration</strong>).</p>
<h2 id="openai-gym">OpenAI Gym</h2>
<p>A great tool for RL is the <a href="https://gym.openai.com/">OpenAI Gym</a> - a <strong>simulation environment</strong>, which can simulate many different environments starting from Atari games, to the physics behind pole balancing. It is one of the most popular simulation environments for training reinforcement learning algorithms, and is maintained by <a href="https://openai.com/">OpenAI</a>.</p>
<blockquote>
<p><strong>Note</strong>: You can see all the environments available from OpenAI Gym <a href="https://gym.openai.com/envs/#classic_control">here</a>.</p>
</blockquote>
<h2 id="cartpole-balancing">CartPole Balancing</h2>
<p>You have probably all seen modern balancing devices such as the <em>Segway</em> or <em>Gyroscooters</em>. They are able to automatically balance by adjusting their wheels in response to a signal from an accelerometer or gyroscope. In this section, we will learn how to solve a similar problem - balancing a pole. It is similar to a situation when a circus performer needs to balance a pole on his hand - but this pole balancing only occurs in 1D.</p>
<p>A simplified version of balancing is known as a <strong>CartPole</strong> problem. In the cartpole world, we have a horizontal slider that can move left or right, and the goal is to balance a vertical pole on top of the slider as it moves.</p>
<p><img alt="a cartpole" src="images/cartpole.png" width="200"/></p>
<p>To create and use this environment, we need a couple of lines of Python code:</p>
<pre><code class="language-python">import gym
env = gym.make(&quot;CartPole-v1&quot;)

env.reset()
done = False
total_reward = 0
while not done:
   env.render()
   action = env.action_space.sample()
   observaton, reward, done, info = env.step(action)
   total_reward += reward

print(f&quot;Total reward: {total_reward}&quot;)
</code></pre>
<p>Each environment can be accessed exactly in the same way:
* <code>env.reset</code> starts a new experiment
* <code>env.step</code> performs a simulation step. It receives an <strong>action</strong> from the <strong>action space</strong>, and returns an <strong>observation</strong> (from the observation space), as well as a reward and a termination flag.</p>
<p>In the example above we perform a random action at each step, which is why the experiment life is very short:</p>
<p><img alt="non-balancing cartpole" src="images/cartpole-nobalance.gif" /></p>
<p>The goal of a RL algorithm is to train a model - the so called <strong>policy</strong> &pi; - which will return the action in response to a given state. We can also consider policy to be probabilistic, eg. for any state <em>s</em> and action <em>a</em> it will return the probability &pi;(<em>a</em>|<em>s</em>) that we should take <em>a</em> in state <em>s</em>.</p>
<h2 id="policy-gradients-algorithm">Policy Gradients Algorithm</h2>
<p>The most obvious way to model a policy is by creating a neural network that will take states as input, and return corresponding actions (or rather the probabilities of all actions). In a sense, it would be similar to a normal classification task, with a major difference - we do not know in advance which actions should we take at each of the steps.</p>
<p>The idea here is to estimate those probabilities. We build a vector of <strong>cumulative rewards</strong> which shows our total reward at each step of the experiment. We also apply <strong>reward discounting</strong> by multiplying earlier rewards by some coefficient &gamma;=0.99, in order to diminish the role of earlier rewards. Then, we reinforce those steps along the experiment path that yield larger rewards.</p>
<blockquote>
<p>Learn more about the Policy Gradient algorithm and see it in action in the <a href="CartPole-RL-TF.ipynb">example notebook</a>.</p>
</blockquote>
<h2 id="actor-critic-algorithm">Actor-Critic Algorithm</h2>
<p>An improved version of the Policy Gradients approach is called <strong>Actor-Critic</strong>. The main idea behind it is that the neural network would be trained to return two things:</p>
<ul>
<li>The policy, which determines which action to take. This part is called <strong>actor</strong></li>
<li>The estimation of the total reward we can expect to get at this state - this part is called <strong>critic</strong>.</li>
</ul>
<p>In a sense, this architecture resembles a <a href="../../4-ComputerVision/10-GANs/README_chs/">GAN</a>, where we have two networks that are trained against each other. In the actor-critic model, the actor proposes the action we need to take, and the critic tries to be critical and estimate the result. However, our goal is to train those networks in unison.</p>
<p>Because we know both the real cumulative rewards and the results returned by the critic during the experiment, it is relatively easy to build a loss function that will minimize the difference between them. That would give us <strong>critic loss</strong>. We can compute <strong>actor loss</strong> by using the same approach as in the policy gradient algorithm.</p>
<p>After running one of those algorithms, we can expect our CartPole to behave like this:</p>
<p><img alt="a balancing cartpole" src="images/cartpole-balance.gif" /></p>
<h2 id="exercises-policy-gradients-and-actor-critic-rl">âœï¸ Exercises: Policy Gradients and Actor-Critic RL</h2>
<p>Continue your learning in the following notebooks:</p>
<ul>
<li><a href="CartPole-RL-TF.ipynb">RL in TensorFlow</a></li>
<li><a href="CartPole-RL-PyTorch.ipynb">RL in PyTorch</a></li>
</ul>
<h2 id="other-rl-tasks">Other RL Tasks</h2>
<p>Reinforcement Learning nowadays is a fast growing field of research. Some of the interesting examples of reinforcement learning are:</p>
<ul>
<li>Teaching a computer to play <strong>Atari Games</strong>. The challenging part in this problem is that we do not have simple state represented as a vector, but rather a screenshot - and we need to use the CNN to convert this screen image to a feature vector, or to extract reward information. Atari games are available in the Gym.</li>
<li>Teaching a computer to play board games, such as Chess and Go. Recently state-of-the-art programs like <strong>Alpha Zero</strong> were trained from scratch by two agents playing against each other, and improving at each step.</li>
<li>In industry, RL is used to create control systems from simulation. A service called <a href="https://azure.microsoft.com/services/project-bonsai/?WT.mc_id=academic-77998-cacaste">Bonsai</a> is specifically designed for that.</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>We have now learned how to train agents to achieve good results just by providing them a reward function that defines the desired state of the game, and by giving them an opportunity to intelligently explore the search space. We have successfully tried two algorithms, and achieved a good result in a relatively short period of time. However, this is just the beginning of your journey into RL, and you should definitely consider taking a separate course is you want to dig deeper.</p>
<h2 id="challenge">ğŸš€ Challenge</h2>
<p>Explore the applications listed in the 'Other RL Tasks' section and try to implement one!</p>
<h2 id="post-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/222">Post-lecture quiz</a></h2>
<h2 id="review-self-study">Review &amp; Self Study</h2>
<p>Learn more about classical reinforcement learning in our <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README_chs.md">Machine Learning for Beginners Curriculum</a>.</p>
<p>Watch <a href="https://www.youtube.com/watch?v=qv6UVOQ0F44">this great video</a> talking about how a computer can learn to play Super Mario.</p>
<h2 id="assignment-train-a-mountain-car">Assignment: <a href="lab/README_chs/">Train a Mountain Car</a></h2>
<p>Your goal during this assignment would be to train a different Gym environment - <a href="https://www.gymlibrary.ml/environments/classic_control/mountain_car/">Mountain Car</a>.</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
