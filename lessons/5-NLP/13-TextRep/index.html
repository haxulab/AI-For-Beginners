<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Representing Text as Tensors - 人工智能初学者课程</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Representing Text as Tensors";
        var mkdocs_page_input_path = "lessons/5-NLP/13-TextRep/README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> 人工智能初学者课程
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../README_chs/">课程介绍</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">课程设置</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/for-teachers_chs/">给教育者</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/how-to-run_chs/">如何运行代码</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0-course-setup/setup_chs/">开始使用这个课程</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">人工智能简介</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/README_chs/">人工智能简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../1-Intro/assignment_chs/">游戏开发马拉松</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">知识表示与专家系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/README_chs/">知识表示与专家系统</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2-Symbolic/assignment_chs/">构建一个本体</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">神经网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/README_chs/">神经网络介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/03-Perceptron/README_chs/">神经网络入门：感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/04-OwnFramework/README_chs/">神经网络介绍. 多层感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../3-NeuralNetworks/05-Frameworks/README_chs/">神经网络框架</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">计算机视觉</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/06-IntroCV/README_chs/">计算机视觉简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >卷积神经网络</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/07-ConvNets/CNN_Architectures_chs/">著名的 CNN 架构</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/07-ConvNets/README_chs/">卷积神经网络</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >迁移学习</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/08-TransferLearning/README_chs/">预训练网络和迁移学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../4-ComputerVision/08-TransferLearning/TrainingTricks_chs/">深度学习训练技巧</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/09-Autoencoders/README_chs/">自动编码器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/10-GANs/README_chs/">生成对抗网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/11-ObjectDetection/README_chs/">目标检测</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4-ComputerVision/12-Segmentation/README_chs/">分割</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">自然语言处理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="README_chs/">将文本表示为张量</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../14-Embeddings/README_chs/">嵌入</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../15-LanguageModeling/README_chs/">语言建模</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../16-RNN/README_chs/">循环神经网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../17-GenerativeNetworks/README_chs/">生成网络</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../18-Transformers/READMEtransformers_chs">注意机制和Transformer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../19-NER/README_chs/">命名实体识别</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../20-LangModels/READMELargeLang_chs">预训练大语言模型</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">其他技术</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/21-GeneticAlgorithms/README_chs/">遗传算法</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/22-DeepRL/README_chs/">深度强化学习</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../6-Other/23-MultiagentSystems/README_chs/">多智能体系统</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">AI伦理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../7-Ethics/README_chs/">道德与负责任的人工智能</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">多模态网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../X-Extras/X1-MultiModal/README_chs/">多模态网络</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">人工智能初学者课程</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Representing Text as Tensors</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="representing-text-as-tensors">Representing Text as Tensors</h1>
<h2 id="pre-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/113">Pre-lecture quiz</a></h2>
<h2 id="text-classification">Text Classification</h2>
<p>Throughout the first part of this section, we will focus on <strong>text classification</strong> task. We will use the <a href="https://www.kaggle.com/amananandrai/ag-news-classification-dataset">AG News</a> Dataset, which contains news articles like the following:</p>
<ul>
<li>Category: Sci/Tech</li>
<li>Title: Ky. Company Wins Grant to Study Peptides (AP)</li>
<li>Body: AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop...</li>
</ul>
<p>Our goal will be to classify the news item into one of the categories based on text.</p>
<h2 id="representing-text">Representing text</h2>
<p>If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors. Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8.</p>
<p><img alt="Image showing diagram mapping a character to an ASCII and binary representation" src="images/ascii-character-map.png" width="50%"/></p>
<blockquote>
<p><a href="https://www.seobility.net/en/wiki/ASCII">Image source</a></p>
</blockquote>
<p>As humans, we understand what each letter <strong>represents</strong>, and how all characters come together to form the words of a sentence. However, computers by themselves do not have such an understanding, and neural network has to learn the meaning during training.</p>
<p>Therefore, we can use different approaches when representing text:</p>
<ul>
<li><strong>Character-level representation</strong>, when we represent text by treating each character as a number. Given that we have <em>C</em> different characters in our text corpus, the word <em>Hello</em> would be represented by 5x<em>C</em> tensor. Each letter would correspond to a tensor column in one-hot encoding.</li>
<li><strong>Word-level representation</strong>, in which we create a <strong>vocabulary</strong> of all words in our text, and then represent words using one-hot encoding. This approach is somehow better, because each letter by itself does not have much meaning, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network. However, given the large dictionary size, we need to deal with high-dimensional sparse tensors.</li>
</ul>
<p>Regardless of the representation, we first need to convert the text into a sequence of <strong>tokens</strong>, one token being either a character, a word, or sometimes even part of a word. Then, we convert the token into a number, typically using <strong>vocabulary</strong>, and this number can be fed into a neural network using one-hot encoding.</p>
<h2 id="n-grams">N-Grams</h2>
<p>In natural language, precise meaning of words can only be determined in context. For example, meanings of <em>neural network</em> and <em>fishing network</em> are completely different. One of the ways to take this into account is to build our model on pairs of words, and considering word pairs as separate vocabulary tokens. In this way, the sentence <em>I like to go fishing</em> will be represented by the following sequence of tokens: <em>I like</em>, <em>like to</em>, <em>to go</em>, <em>go fishing</em>. The problem with this approach is that the dictionary size grows significantly, and combinations like <em>go fishing</em> and <em>go shopping</em> are presented by different tokens, which do not share any semantic similarity despite the same verb.  </p>
<p>In some cases, we may consider using tri-grams -- combinations of three words -- as well. Thus the approach is such is often called <strong>n-grams</strong>. Also, it makes sense to use n-grams with character-level representation, in which case n-grams will roughly correspond to different syllabi.</p>
<h2 id="bag-of-words-and-tfidf">Bag-of-Words and TF/IDF</h2>
<p>When solving tasks like text classification, we need to be able to represent text by one fixed-size vector, which we will use as an input to final dense classifier. One of the simplest ways to do that is to combine all individual word representations, eg. by adding them. If we add one-hot encodings of each word, we will end up with a vector of frequencies, showing how many times each word appears inside the text. Such representation of text is called <strong>bag of words</strong> (BoW).</p>
<p><img src="images/bow.png" width="90%"/></p>
<blockquote>
<p>Image by the author</p>
</blockquote>
<p>A BoW essentially represents which words appear in text and in which quantities, which can indeed be a good indication of what the text is about. For example, news article on politics is likely to contains words such as <em>president</em> and <em>country</em>, while scientific publication would have something like <em>collider</em>, <em>discovered</em>, etc. Thus, word frequencies can in many cases be a good indicator of text content.</p>
<p>The problem with BoW is that certain common words, such as <em>and</em>, <em>is</em>, etc. appear in most of the texts, and they have highest frequencies, masking out the words that are really important. We may lower the importance of those words by taking into account the frequency at which words occur in the whole document collection. This is the main idea behind TF/IDF approach, which is covered in more detail in the notebooks attached to this lesson.</p>
<p>However, none of those approaches can fully take into account the <strong>semantics</strong> of text. We need more powerful neural networks models to do this, which we will discuss later in this section.</p>
<h2 id="exercises-text-representation">✍️ Exercises: Text Representation</h2>
<p>Continue your learning in the following notebooks:</p>
<ul>
<li><a href="TextRepresentationPyTorch.ipynb">Text Representation with PyTorch</a></li>
<li><a href="TextRepresentationTF.ipynb">Text Representation with TensorFlow</a></li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>So far, we have studied techniques that can add frequency weight to different words. They are, however, unable to represent meaning or order. As the famous linguist J. R. Firth said in 1935, "The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously." We will learn later in the course how to capture contextual information from text using language modeling.</p>
<h2 id="challenge">🚀 Challenge</h2>
<p>Try some other exercises using bag-of-words and different data models. You might be inspired by this <a href="https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words">competition on Kaggle</a></p>
<h2 id="post-lecture-quiz"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/213">Post-lecture quiz</a></h2>
<h2 id="review-self-study">Review &amp; Self Study</h2>
<p>Practice your skills with text embeddings and bag-of-words techniques on <a href="https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste">Microsoft Learn</a></p>
<h2 id="assignment-notebooks"><a href="assignment_chs/">Assignment: Notebooks</a></h2>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
