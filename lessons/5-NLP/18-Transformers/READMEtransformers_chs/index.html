<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../../img/favicon.ico" />
    <title>注意机制和Transformer - 人工智能初学者课程</title>
    <link rel="stylesheet" href="../../../../css/theme.css" />
    <link rel="stylesheet" href="../../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "\u6ce8\u610f\u673a\u5236\u548cTransformer";
        var mkdocs_page_input_path = "lessons/5-NLP/18-Transformers/READMEtransformers_chs.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../.." class="icon icon-home"> 人工智能初学者课程
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../../README_chs/">课程介绍</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">课程设置</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../0-course-setup/for-teachers_chs/">给教育者</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../0-course-setup/how-to-run_chs/">如何运行代码</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../0-course-setup/setup_chs/">开始使用这个课程</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">人工智能简介</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../1-Intro/README_chs/">人工智能简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../1-Intro/assignment_chs/">游戏开发马拉松</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">知识表示与专家系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../2-Symbolic/README_chs/">知识表示与专家系统</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../2-Symbolic/assignment_chs/">构建一个本体</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">神经网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../3-NeuralNetworks/README_chs/">神经网络介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../3-NeuralNetworks/03-Perceptron/README_chs/">神经网络入门：感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../3-NeuralNetworks/04-OwnFramework/README_chs/">神经网络介绍. 多层感知器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../3-NeuralNetworks/05-Frameworks/README_chs/">神经网络框架</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">计算机视觉</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../4-ComputerVision/06-IntroCV/README_chs/">计算机视觉简介</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >卷积神经网络</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../4-ComputerVision/07-ConvNets/CNN_Architectures_chs/">著名的 CNN 架构</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../4-ComputerVision/07-ConvNets/README_chs/">卷积神经网络</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >迁移学习</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../4-ComputerVision/08-TransferLearning/README_chs/">预训练网络和迁移学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../4-ComputerVision/08-TransferLearning/TrainingTricks_chs/">深度学习训练技巧</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../4-ComputerVision/09-Autoencoders/README_chs/">自动编码器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../4-ComputerVision/10-GANs/README_chs/">生成对抗网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../4-ComputerVision/11-ObjectDetection/README_chs/">目标检测</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../4-ComputerVision/12-Segmentation/README_chs/">分割</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">自然语言处理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../13-TextRep/README_chs/">将文本表示为张量</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../14-Embeddings/README_chs/">嵌入</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../15-LanguageModeling/README_chs/">语言建模</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../16-RNN/README_chs/">循环神经网络</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../17-GenerativeNetworks/README_chs/">生成网络</a>
                  </li>
                  <li class="toctree-l1"><a class="" href=".">注意机制和Transformer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../19-NER/README_chs/">命名实体识别</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../20-LangModels/READMELargeLang_chs">预训练大语言模型</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">其他技术</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../6-Other/21-GeneticAlgorithms/README_chs/">遗传算法</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../6-Other/22-DeepRL/README_chs/">深度强化学习</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../6-Other/23-MultiagentSystems/README_chs/">多智能体系统</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">AI伦理</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../7-Ethics/README_chs/">道德与负责任的人工智能</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">多模态网络</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../X-Extras/X1-MultiModal/README_chs/">多模态网络</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../..">人工智能初学者课程</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">注意机制和Transformer</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="transformer">注意机制和Transformer</h1>
<h2 id="_1"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/118">讲座前测验</a></h2>
<p>自然语言处理领域中最重要的问题之一是<strong>机器翻译</strong>，这一任务是诸如谷歌翻译等工具的基础。在本节中，我们将重点关注机器翻译，或更广泛地说，任何<em>序列到序列</em>任务（也称为<strong>句子转换</strong>）。</p>
<p>在RNN中，序列到序列是由两个递归网络实现的，其中一个网络，即<strong>编码器</strong>，将输入序列压缩成一个隐藏状态，而另一个网络，即<strong>解码器</strong>，将这个隐藏状态展开成一个翻译结果。这种方法有几个问题：</p>
<ul>
<li>编码器网络的最终状态难以记住句子的开头，从而导致长句子的模型质量较差</li>
<li>序列中的所有单词对结果的影响相同。然而，实际上，输入序列中的特定单词对顺序输出的影响往往比其他单词大。</li>
</ul>
<p><strong>注意机制</strong>提供了一种给每个输入向量对RNN每个输出预测的上下文影响进行加权的方法。其实现方式是通过在输入RNN和输出RNN的中间状态之间创建捷径。这样，在生成输出符号y<sub>t</sub>时，我们将考虑所有输入的隐藏状态h<sub>i</sub>，但有不同的权重系数&alpha;<sub>t,i</sub>。</p>
<p><img alt="显示带有加性注意层的编码器/解码器模型的图像" src="../images/encoder-decoder-attention.png" /></p>
<blockquote>
<p><a href="https://arxiv.org/pdf/1409.0473.pdf">Bahdanau et al., 2015</a> 中的加性注意机制编码器-解码器模型，此图来自<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">这篇博客文章</a></p>
</blockquote>
<p>注意矩阵{&alpha;<sub>i,j</sub>}表示某些输入词在生成输出序列中特定词时的作用大小。下面是一个这样的矩阵示例：</p>
<p><img alt="显示由RNNsearch-50找到的示例对齐结果的图像，取自Bahdanau - arviz.org" src="../images/bahdanau-fig3.png" /></p>
<blockquote>
<p>图来自<a href="https://arxiv.org/pdf/1409.0473.pdf">Bahdanau et al., 2015</a> (图3)</p>
</blockquote>
<p>注意机制在当前或最近的自然语言处理技术中起到了至关重要的作用。但是，添加注意机制大大增加了模型参数的数量，这导致了RNN的扩展问题。扩展RNN的一个关键约束是模型的递归性质使得批处理和并行训练变得具有挑战性。在RNN中，每个序列元素需要按顺序处理，这意味着它不能轻易并行化。</p>
<p><img alt="带注意机制的编码器解码器" src="../images/EncDecAttention.gif" /></p>
<blockquote>
<p><a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">谷歌博客</a>中的图</p>
</blockquote>
<p>结合这种限制，注意机制的采用导致了我们今天所知并使用的最先进的Transformer模型的创建，包括BERT和Open-GPT3等。</p>
<h2 id="transformer_1">Transformer模型</h2>
<p>Transformer背后的主要想法之一是避免RNN的顺序性质，并创建一个在训练期间可并行化的模型。这是通过实现两个想法来实现的：</p>
<ul>
<li>位置编码</li>
<li>使用自注意机制来捕捉模式，而不是RNN（或CNN）（这就是介绍Transformer的论文被称为<em><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></em>的原因）</li>
</ul>
<h3 id="_2">位置编码/嵌入</h3>
<p>位置编码的想法如下。
1. 使用RNN时，标记的相对位置由步数表示，因此不需要显式表示。
2. 但是，一旦我们切换到注意机制，我们需要知道标记在序列中的相对位置。
3. 为了获得位置编码，我们将标记序列与标记在序列中的位置序列（即一系列数字0,1, ...）进行扩展。
4. 然后我们将标记位置与标记嵌入向量混合。为了将位置（整数）转换为向量，我们可以使用不同的方法：</p>
<ul>
<li>可训练嵌入，类似于标记嵌入。这是我们考虑的方法。我们在标记和它们的位置上应用嵌入层，得到相同维度的嵌入向量，然后将它们加在一起。</li>
<li>固定位置编码函数，如原始论文所提议的。</li>
</ul>
<p><img alt="位置编码示意图" src="../images/pos-embedding.png" /></p>
<blockquote>
<p>作者提供的图片</p>
</blockquote>
<p>通过位置嵌入，我们得到的结果既嵌入了原始标记，也嵌入了其在序列中的位置。</p>
<h3 id="_3">多头自注意</h3>
<p>接下来，我们需要捕捉序列中的一些模式。为了做到这一点，Transformer使用了<strong>自注意</strong>机制，这本质上是将注意机制应用于相同的输入和输出序列。应用自注意机制使我们能够考虑句子中的<strong>上下文</strong>，并查看哪些词是相互关联的。例如，它允许我们查看哪些词是代词（如<em>它</em>）所指的，并且也考虑上下文：</p>
<p><img alt="" src="../images/CoreferenceResolution.png" /></p>
<blockquote>
<p>图片来自<a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html">谷歌博客</a></p>
</blockquote>
<p>在Transformer中，我们使用<strong>多头注意</strong>，以使网络能够捕捉几种不同类型的依赖关系，例如长时与短时词关系，共指关系与其他等等。</p>
<p><a href="../TransformersTF.ipynb">TensorFlow笔记本</a>包含关于Transformer层实现的更多详细信息。</p>
<h3 id="-">编码器-解码器注意</h3>
<p>在Transformer中，注意机制在两个地方使用：</p>
<ul>
<li>使用自注意机制捕捉输入文本中的模式</li>
<li>执行序列翻译——这是编码器和解码器之间的注意层。</li>
</ul>
<p>编码器-解码器注意非常类似于在本节开头描述的RNN中使用的注意机制。此动画图解释了编码器-解码器注意的作用。</p>
<p><img alt="展示Transformer模型中评估过程的动画GIF。" src="../images/transformer-animated-explanation.gif" /></p>
<p>由于每个输入位置独立地映射到每个输出位置，Transformer比RNN更能并行化，这使得更大且更具表现力的语言模型成为可能。每个注意头可以用来学习不同的单词关系，从而提高自然语言处理任务的效果。</p>
<h2 id="bert">BERT</h2>
<p><strong>BERT</strong>（双向编码器表示的Transformer）是一个非常大的多层Transformer网络，<em>BERT-base</em>有12层，<em>BERT-large</em>有24层。该模型首先在大规模文本数据集（维基百科+书籍）上进行无监督训练（预测句子中的遮蔽词）。在预训练期间，模型吸收了大量的语言理解，这可以在其他数据集上通过微调进行充分利用。这个过程被称为<strong>迁移学习</strong>。</p>
<p><img alt="图片来自http://jalammar.github.io/illustrated-bert/" src="../images/jalammarBERT-language-modeling-masked-lm.png" /></p>
<blockquote>
<p>图片<a href="http://jalammar.github.io/illustrated-bert/">来源</a></p>
</blockquote>
<h2 id="transformer_2">✍️ 练习：Transformer</h2>
<p>在以下笔记本中继续学习：</p>
<ul>
<li><a href="../TransformersPyTorch.ipynb">PyTorch中的Transformer</a></li>
<li><a href="../TransformersTF.ipynb">TensorFlow中的Transformer</a></li>
</ul>
<h2 id="_4">结论</h2>
<p>在本课中，你了解了Transformer和注意机制，这些都是NLP工具箱中的基本工具。Transformer架构有很多变体，包括BERT、DistilBERT、BigBird、OpenGPT3等，可以进行微调。<a href="https://github.com/huggingface/">HuggingFace包</a>提供了使用PyTorch和TensorFlow训练这些架构的资源库。</p>
<h2 id="_5">🚀 挑战</h2>
<h2 id="_6"><a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/218">课后测验</a></h2>
<h2 id="_7">复习和自学</h2>
<ul>
<li><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/">博客文章</a>，解释经典的<a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>论文，关于Transformer。</li>
<li><a href="https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452">一系列博客文章</a>，详细解释Transformer的架构。</li>
</ul>
<h2 id="_8"><a href="../assignment_chs/">作业</a></h2>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../../..";</script>
    <script src="../../../../js/theme_extra.js"></script>
    <script src="../../../../js/theme.js"></script>
      <script src="../../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
